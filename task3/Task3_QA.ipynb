{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc651209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load QA pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "# Context\n",
    "context = \"\"\"\n",
    "Natural Language Processing (NLP) is a subfield of deep learning that enables machines to understand and generate human language. \n",
    "It has two main categories: Natural Language Understanding and Natural Language Generation. Applications include text processing, conversational systems, sentiment analysis, information extraction, and speech applications.\n",
    "\n",
    "The general process starts with data collection and storage, followed by preprocessing. Preprocessing steps include tokenization (splitting text into smaller units), lowercasing and removing stop words, lemmatization (mapping words to their base form, more accurate but computationally heavier), and stemming (rule-based truncation to stems, faster but less accurate). These steps are known as text normalization.\n",
    "\n",
    "Text representation methods convert words into numerical formats. Traditional count-based methods include:\n",
    "- One-hot encoding: represents each word as a binary vector, simple but memory-intensive and does not capture semantics.\n",
    "- Bag of Words (BoW): represents documents by word frequencies. Easy to implement but ignores context and order.\n",
    "- TF-IDF: balances word frequency within a document with how rare the word is across documents. Useful for classification and keyword extraction, but still lacks semantic understanding.\n",
    "\n",
    "Neural embeddings capture meaning and context:\n",
    "- Word2Vec: CBOW predicts a word from context, fast but less accurate for rare words. Skip-Gram predicts context words from a target word, better for rare words but slower.\n",
    "- GloVe: uses global co-occurrence statistics to create vectors; for example, king - man + woman â‰ˆ queen.\n",
    "- FastText: breaks words into subword units, allowing embeddings for unseen words or misspellings.\n",
    "\n",
    "Contextual embeddings like BERT use transformers to assign relevance to each word in a sentence, producing context-dependent word meanings (e.g., 'bank' in 'river bank' vs 'bank account').\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    q = input(\"Question (type 'exit' to quit): \")\n",
    "    if q.lower() == \"exit\":\n",
    "        break\n",
    "    if q.endswith(\"?\"):\n",
    "        print(f\"Question: {q}\")\n",
    "        print(f\"Answer: {qa_pipeline(question=q, context=context)['answer']}\") # type: ignore\n",
    "    else:\n",
    "        print(\"Please ask a valid question ending with '?'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b397faf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
